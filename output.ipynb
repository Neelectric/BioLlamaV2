{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a5058c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:31:43.495723Z",
     "iopub.status.busy": "2024-04-09T18:31:43.495589Z",
     "iopub.status.idle": "2024-04-09T18:31:43.674103Z",
     "shell.execute_reply": "2024-04-09T18:31:43.673800Z"
    },
    "papermill": {
     "duration": 0.185095,
     "end_time": "2024-04-09T18:31:43.674890",
     "exception": false,
     "start_time": "2024-04-09T18:31:43.489795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export HF_HOME=../hf_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ada1744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:31:43.684961Z",
     "iopub.status.busy": "2024-04-09T18:31:43.684860Z",
     "iopub.status.idle": "2024-04-09T18:31:46.103288Z",
     "shell.execute_reply": "2024-04-09T18:31:46.102953Z"
    },
    "papermill": {
     "duration": 2.42413,
     "end_time": "2024-04-09T18:31:46.104335",
     "exception": false,
     "start_time": "2024-04-09T18:31:43.680205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (0.29.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (23.2)\r\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (5.9.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (2.2.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (0.4.2)\r\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from huggingface-hub->accelerate) (4.65.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2.0.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git config --global credential.helper store\n",
    "!pip install accelerate\n",
    "!git config --global user.name \"Neelectric\"\n",
    "!git config --global user.email \"Neel.R@web.de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9c523a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:31:46.115093Z",
     "iopub.status.busy": "2024-04-09T18:31:46.114983Z",
     "iopub.status.idle": "2024-04-09T18:31:54.288703Z",
     "shell.execute_reply": "2024-04-09T18:31:54.288388Z"
    },
    "papermill": {
     "duration": 8.179295,
     "end_time": "2024-04-09T18:31:54.289505",
     "exception": false,
     "start_time": "2024-04-09T18:31:46.110210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/nfs/primary/BioLlamaV2/wandb/run-20240409_183150-evzw4pzs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meager-sun-13\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_v2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/neelectric/biollama_v2/runs/evzw4pzs/workspace\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "wandb.init(project=\"biollama_v2\", # the project I am working on\n",
    "           tags=[\"hf_sft\", \"BioLlamaV2\"]) # the Hyperparameters I want to keep track of\n",
    "\n",
    "wandb.alert(\n",
    "    title=\"Initialising training run\",\n",
    "    text=f\"We have started training\",\n",
    "    level=AlertLevel.WARN,\n",
    "    wait_duration=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40165b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:31:54.299743Z",
     "iopub.status.busy": "2024-04-09T18:31:54.299500Z",
     "iopub.status.idle": "2024-04-09T18:32:02.578805Z",
     "shell.execute_reply": "2024-04-09T18:32:02.578549Z"
    },
    "papermill": {
     "duration": 8.285119,
     "end_time": "2024-04-09T18:32:02.579465",
     "exception": false,
     "start_time": "2024-04-09T18:31:54.294346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported...\n",
      "4.39.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "print(\"imported...\")\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785f9a0",
   "metadata": {
    "papermill": {
     "duration": 0.004426,
     "end_time": "2024-04-09T18:32:02.589318",
     "exception": false,
     "start_time": "2024-04-09T18:32:02.584892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we pull the huggingface token from the local config. Remember to make a copy of config_default.yml, name it config.yml and add your huggingface token. config.yml is in the gitignore so unless you misspell config.yml, it should not get pushed to github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51e0632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:02.599272Z",
     "iopub.status.busy": "2024-04-09T18:32:02.599098Z",
     "iopub.status.idle": "2024-04-09T18:32:02.898407Z",
     "shell.execute_reply": "2024-04-09T18:32:02.898101Z"
    },
    "papermill": {
     "duration": 0.304958,
     "end_time": "2024-04-09T18:32:02.899062",
     "exception": false,
     "start_time": "2024-04-09T18:32:02.594104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# token = input(f\"Enter token: \")\n",
    "from box import Box\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = Box.from_yaml(f.read())\n",
    "token = config.secrets.hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ec4a0",
   "metadata": {
    "papermill": {
     "duration": 0.0047,
     "end_time": "2024-04-09T18:32:02.908823",
     "exception": false,
     "start_time": "2024-04-09T18:32:02.904123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We also need to verfiy whether the GPU supports bfloat16 (less precision but more range). If not (for example if running on TITAN GPUs), we use float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01f8c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:02.918491Z",
     "iopub.status.busy": "2024-04-09T18:32:02.918205Z",
     "iopub.status.idle": "2024-04-09T18:32:03.006565Z",
     "shell.execute_reply": "2024-04-09T18:32:03.006317Z"
    },
    "papermill": {
     "duration": 0.093834,
     "end_time": "2024-04-09T18:32:03.007178",
     "exception": false,
     "start_time": "2024-04-09T18:32:02.913344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_bf16_supported():\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "torch_dtype = torch.float32\n",
    "print(f\"Using dtype: {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a6cfe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:03.017956Z",
     "iopub.status.busy": "2024-04-09T18:32:03.017729Z",
     "iopub.status.idle": "2024-04-09T18:32:14.591306Z",
     "shell.execute_reply": "2024-04-09T18:32:14.590906Z"
    },
    "papermill": {
     "duration": 11.579489,
     "end_time": "2024-04-09T18:32:14.591981",
     "exception": false,
     "start_time": "2024-04-09T18:32:03.012492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started script\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                             | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                  | 1/2 [00:06<00:06,  6.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"started script\")\n",
    "\n",
    "llama_path = \"h2oai/h2ogpt-4096-llama2-7b-chat\"\n",
    "# llama_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# llama_path = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_path, \n",
    "                                          token = token,\n",
    "                                          cache_dir = \"../hf_cache/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_path, \n",
    "                                             token = token,\n",
    "                                            #  device_map = \"auto\",\n",
    "                                            device_map = \"cuda:0\",\n",
    "                                             torch_dtype = torch_dtype,\n",
    "                                             cache_dir = \"../hf_cache/\")\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0a73b23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:14.604100Z",
     "iopub.status.busy": "2024-04-09T18:32:14.603911Z",
     "iopub.status.idle": "2024-04-09T18:32:21.962853Z",
     "shell.execute_reply": "2024-04-09T18:32:21.962454Z"
    },
    "papermill": {
     "duration": 7.365457,
     "end_time": "2024-04-09T18:32:21.963588",
     "exception": false,
     "start_time": "2024-04-09T18:32:14.598131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question.\n",
      "Low insulin to glucagon ratio is not seen in:\n",
      "(A) Glycogen synthesis\n",
      "(B) Glycogen breakdown\n",
      "(C) Gluconeogenesis\n",
      "(D) Ketogenesis\n",
      "Answer. \n",
      "(D) Ketogenesis\n",
      "\n",
      "Explanation:\n",
      "A low insulin to glucagon ratio is typically seen in states of ketosis, where the body is relying on ketones for energy instead of glucose. Therefore, option (D) Ketogenesis is the correct answer.\n",
      "Glycogen synthesis (A) and glycogen breakdown (B) are processes that occur in the liver and are regulated by insulin and glucagon, respectively. Gluconeogenesis (C) is the process by which the liver and kidneys produce glucose from non-carbohydrate sources, such as amino ac\n",
      "newly generated 150\n",
      "20.40309322331401 t/s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "max_new_tokens = 150\n",
    "prompt = \"In the era of generative AI, \"\n",
    "medmcqa2 = \"\"\"\n",
    "Question.\n",
    "Low insulin to glucagon ratio is not seen in:\n",
    "(A) Glycogen synthesis\n",
    "(B) Glycogen breakdown\n",
    "(C) Gluconeogenesis\n",
    "(D) Ketogenesis\n",
    "Answer. \n",
    "\"\"\"\n",
    "tokenized_prompt = tokenizer.encode(medmcqa2, return_tensors = \"pt\").to('cuda')\n",
    "time_before = time()\n",
    "raw_output = model.generate(tokenized_prompt,\n",
    "                            max_new_tokens = max_new_tokens,\n",
    "                            temperature = 0.01)\n",
    "time_after = time()\n",
    "time_taken = time_after - time_before\n",
    "untokenized_output = tokenizer.decode(raw_output[0], skip_special_tokens = True)\n",
    "num_generated = len(raw_output[0]) - len(tokenized_prompt[0])\n",
    "print(untokenized_output)\n",
    "print(f\"newly generated {num_generated}\")\n",
    "print(f\"{num_generated / time_taken} t/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c57398",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:21.975862Z",
     "iopub.status.busy": "2024-04-09T18:32:21.975755Z",
     "iopub.status.idle": "2024-04-09T18:32:26.569047Z",
     "shell.execute_reply": "2024-04-09T18:32:26.568694Z"
    },
    "papermill": {
     "duration": 4.600165,
     "end_time": "2024-04-09T18:32:26.570099",
     "exception": false,
     "start_time": "2024-04-09T18:32:21.969934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/primary/pubmed_cleaned/\n",
      "['abs_1_14.tsv', 'abs_1_15.tsv', 'abs_1_23.tsv', 'abs_1_0.tsv', 'abs_1_30.tsv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/primary/pubmed_cleaned/abs_1_14.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + \"/pubmed_cleaned/\"\n",
    "print(path)\n",
    "files = os.listdir(path)\n",
    "print(files[:5])\n",
    "# load in the first file and print first 20 lines\n",
    "# with open(path + files[0], \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines[:20]:\n",
    "#         print(line)\n",
    "from datasets import load_dataset\n",
    "print(path + files[0])\n",
    "dataset = load_dataset(\"text\", \n",
    "                       cache_dir = \"../hf_cache/\",\n",
    "                       data_files = path + files[0], \n",
    "                       split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f10feb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:26.582502Z",
     "iopub.status.busy": "2024-04-09T18:32:26.582212Z",
     "iopub.status.idle": "2024-04-09T18:32:26.585588Z",
     "shell.execute_reply": "2024-04-09T18:32:26.585377Z"
    },
    "papermill": {
     "duration": 0.010157,
     "end_time": "2024-04-09T18:32:26.586214",
     "exception": false,
     "start_time": "2024-04-09T18:32:26.576057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 67773\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f607d69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:26.599611Z",
     "iopub.status.busy": "2024-04-09T18:32:26.599359Z",
     "iopub.status.idle": "2024-04-09T18:32:26.604149Z",
     "shell.execute_reply": "2024-04-09T18:32:26.603894Z"
    },
    "papermill": {
     "duration": 0.011942,
     "end_time": "2024-04-09T18:32:26.604869",
     "exception": false,
     "start_time": "2024-04-09T18:32:26.592927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 6607.34M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = True\n",
    "model.model.embed_tokens.weight.requires_grad_(False);\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6d9e0",
   "metadata": {
    "papermill": {
     "duration": 0.005654,
     "end_time": "2024-04-09T18:32:26.617081",
     "exception": false,
     "start_time": "2024-04-09T18:32:26.611427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "538b66fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:26.628344Z",
     "iopub.status.busy": "2024-04-09T18:32:26.628076Z",
     "iopub.status.idle": "2024-04-09T18:32:28.976899Z",
     "shell.execute_reply": "2024-04-09T18:32:28.976491Z"
    },
    "papermill": {
     "duration": 2.355299,
     "end_time": "2024-04-09T18:32:28.977859",
     "exception": false,
     "start_time": "2024-04-09T18:32:26.622560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "batch_size = 2\n",
    "total_num_steps = 50\n",
    "output_dir = \"../\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    # per_device_train_batch_size=batch_size,\n",
    "    # per_device_eval_batch_size=batch_size//2,\n",
    "    # bf16=False,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=2,\n",
    "    # max_steps = -1\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=total_num_steps // 6,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f845a071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:28.992382Z",
     "iopub.status.busy": "2024-04-09T18:32:28.992137Z",
     "iopub.status.idle": "2024-04-09T18:32:28.994230Z",
     "shell.execute_reply": "2024-04-09T18:32:28.993964Z"
    },
    "papermill": {
     "duration": 0.009439,
     "end_time": "2024-04-09T18:32:28.994912",
     "exception": false,
     "start_time": "2024-04-09T18:32:28.985473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt(row):\n",
    "    text = row[\"text\"]\n",
    "    return text\n",
    "# create_prompt(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ec44038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:29.006089Z",
     "iopub.status.busy": "2024-04-09T18:32:29.005861Z",
     "iopub.status.idle": "2024-04-09T18:32:41.436975Z",
     "shell.execute_reply": "2024-04-09T18:32:41.436629Z"
    },
    "papermill": {
     "duration": 12.437376,
     "end_time": "2024-04-09T18:32:41.437690",
     "exception": false,
     "start_time": "2024-04-09T18:32:29.000314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 1 examples [00:00,  5.68 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 975 examples [00:00, 2861.70 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 1943 examples [00:00, 3450.67 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 2933 examples [00:00, 3837.05 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 3916 examples [00:01, 4078.78 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 4881 examples [00:01, 3578.78 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 5869 examples [00:01, 4048.15 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 6845 examples [00:01, 4412.61 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 7813 examples [00:01, 4659.72 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 8791 examples [00:02, 4859.35 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 9779 examples [00:02, 5027.43 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 10755 examples [00:02, 5110.05 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 11720 examples [00:02, 5166.86 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 12711 examples [00:02, 4337.09 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 13694 examples [00:03, 4583.42 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 14659 examples [00:03, 4811.32 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 15636 examples [00:03, 4944.69 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 16626 examples [00:03, 5064.90 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 17599 examples [00:03, 5103.90 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 18565 examples [00:04, 5181.77 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 19556 examples [00:04, 5243.74 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 20541 examples [00:04, 5264.98 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 21511 examples [00:04, 4366.90 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 22492 examples [00:04, 4643.30 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 23477 examples [00:05, 4826.05 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 24451 examples [00:05, 4943.35 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 25419 examples [00:05, 5025.62 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 26413 examples [00:05, 5061.96 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 27395 examples [00:05, 5126.58 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 28362 examples [00:06, 5151.33 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 29000 examples [00:06, 4571.21 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 30000 examples [00:06, 4809.53 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 31000 examples [00:06, 4973.84 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 32000 examples [00:06, 5061.39 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 33000 examples [00:07, 4991.96 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 34000 examples [00:07, 5263.66 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 34000 examples [00:08, 3842.96 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    # eval_dataset=test_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=512,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72b801a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T18:32:41.452250Z",
     "iopub.status.busy": "2024-04-09T18:32:41.452144Z"
    },
    "papermill": {
     "duration": 155815.971148,
     "end_time": "2024-04-11T13:49:37.415949",
     "exception": false,
     "start_time": "2024-04-09T18:32:41.444801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    }
   ],
   "source": [
    "#set llama model config use_cache to false!!!\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d4dcf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "#print contents of output_dir\n",
    "!ls -l $output_dir\n",
    "#print full path of output_dir\n",
    "# !pwd $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d85831",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2b438",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['text']\n",
    "        inputs = self.tokenizer(text, \n",
    "                                return_tensors=\"pt\", \n",
    "                                padding=True, \n",
    "                                max_length=10,\n",
    "                                truncation=True)\n",
    "        return inputs\n",
    "text_dataset = TextDataset(dataset, tokenizer)\n",
    "dataloader = DataLoader(text_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618abe4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, dataloader, epochs=5, lr=5e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    print(\"starting training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            inputs = batch.to(device)\n",
    "            print(inputs.input_ids.shape)\n",
    "            print(inputs)\n",
    "            outputs = model(**inputs, labels=inputs.input_ids)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e6cb3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train(model, dataloader, epochs=5, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3fda5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195c413",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 155879.851265,
   "end_time": "2024-04-11T13:49:40.839603",
   "environment_variables": {},
   "exception": null,
   "input_path": "experimentation.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-04-09T18:31:40.988338",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}