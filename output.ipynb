{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2938c1f3",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [9]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0c720e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:06:09.386128Z",
     "iopub.status.busy": "2024-04-13T19:06:09.386022Z",
     "iopub.status.idle": "2024-04-13T19:06:09.566001Z",
     "shell.execute_reply": "2024-04-13T19:06:09.565713Z"
    },
    "papermill": {
     "duration": 0.187224,
     "end_time": "2024-04-13T19:06:09.566892",
     "exception": false,
     "start_time": "2024-04-13T19:06:09.379668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export HF_HOME=../hf_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca6398e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:06:09.577775Z",
     "iopub.status.busy": "2024-04-13T19:06:09.577671Z",
     "iopub.status.idle": "2024-04-13T19:06:12.195119Z",
     "shell.execute_reply": "2024-04-13T19:06:12.194772Z"
    },
    "papermill": {
     "duration": 2.623842,
     "end_time": "2024-04-13T19:06:12.196149",
     "exception": false,
     "start_time": "2024-04-13T19:06:09.572307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (0.29.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (23.2)\r\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (5.9.0)\r\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (2.2.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (0.4.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from huggingface-hub->accelerate) (4.65.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2.0.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git config --global credential.helper store\n",
    "!pip install accelerate\n",
    "!git config --global user.name \"Neelectric\"\n",
    "!git config --global user.email \"Neel.R@web.de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f73b3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:06:12.214711Z",
     "iopub.status.busy": "2024-04-13T19:06:12.214604Z",
     "iopub.status.idle": "2024-04-13T19:06:12.216379Z",
     "shell.execute_reply": "2024-04-13T19:06:12.216184Z"
    },
    "papermill": {
     "duration": 0.00733,
     "end_time": "2024-04-13T19:06:12.216971",
     "exception": false,
     "start_time": "2024-04-13T19:06:12.209641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# from wandb import AlertLevel\n",
    "# wandb.init(project=\"biollama_v2\", # the project I am working on\n",
    "#            tags=[\"hf_sft\", \"BioLlamaV2\"]) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# wandb.alert(\n",
    "#     title=\"Initialising training run\",\n",
    "#     text=f\"We have started training\",\n",
    "#     level=AlertLevel.WARN,\n",
    "#     wait_duration=300,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17742a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:06:12.226469Z",
     "iopub.status.busy": "2024-04-13T19:06:12.226242Z",
     "iopub.status.idle": "2024-04-13T19:06:14.500065Z",
     "shell.execute_reply": "2024-04-13T19:06:14.499670Z"
    },
    "papermill": {
     "duration": 2.279277,
     "end_time": "2024-04-13T19:06:14.500825",
     "exception": false,
     "start_time": "2024-04-13T19:06:12.221548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported...\n",
      "4.39.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "print(\"imported...\")\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294da575",
   "metadata": {
    "papermill": {
     "duration": 0.005001,
     "end_time": "2024-04-13T19:06:14.511537",
     "exception": false,
     "start_time": "2024-04-13T19:06:14.506536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we pull the huggingface token from the local config. Remember to make a copy of config_default.yml, name it config.yml and add your huggingface token. config.yml is in the gitignore so unless you misspell config.yml, it should not get pushed to github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5236d5e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:06:14.522259Z",
     "iopub.status.busy": "2024-04-13T19:06:14.521820Z",
     "iopub.status.idle": "2024-04-13T19:06:14.551633Z",
     "shell.execute_reply": "2024-04-13T19:06:14.551272Z"
    },
    "papermill": {
     "duration": 0.035986,
     "end_time": "2024-04-13T19:06:14.552372",
     "exception": false,
     "start_time": "2024-04-13T19:06:14.516386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# token = input(f\"Enter token: \")\n",
    "from box import Box\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = Box.from_yaml(f.read())\n",
    "token = config.secrets.hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea7fbc",
   "metadata": {
    "papermill": {
     "duration": 0.004746,
     "end_time": "2024-04-13T19:06:14.562006",
     "exception": false,
     "start_time": "2024-04-13T19:06:14.557260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We also need to verfiy whether the GPU supports bfloat16 (less precision but more range). If not (for example if running on TITAN GPUs), we use float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4fa071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:06:14.572167Z",
     "iopub.status.busy": "2024-04-13T19:06:14.571937Z",
     "iopub.status.idle": "2024-04-13T19:06:14.651631Z",
     "shell.execute_reply": "2024-04-13T19:06:14.651238Z"
    },
    "papermill": {
     "duration": 0.08549,
     "end_time": "2024-04-13T19:06:14.652266",
     "exception": false,
     "start_time": "2024-04-13T19:06:14.566776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_bf16_supported():\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "torch_dtype = torch.bfloat16\n",
    "print(f\"Using dtype: {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5adee5d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:06:14.663585Z",
     "iopub.status.busy": "2024-04-13T19:06:14.663477Z",
     "iopub.status.idle": "2024-04-13T19:06:14.665140Z",
     "shell.execute_reply": "2024-04-13T19:06:14.664856Z"
    },
    "papermill": {
     "duration": 0.007853,
     "end_time": "2024-04-13T19:06:14.665713",
     "exception": false,
     "start_time": "2024-04-13T19:06:14.657860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n",
    "# medcpt_path = \"ncbi/MedCPT-Article-Encoder\"\n",
    "# document_tokenizer = AutoTokenizer.from_pretrained(medcpt_path)\n",
    "# document_model = AutoModel.from_pretrained(medcpt_path).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d67f48ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:06:14.676535Z",
     "iopub.status.busy": "2024-04-13T19:06:14.676286Z",
     "iopub.status.idle": "2024-04-13T19:27:00.333715Z",
     "shell.execute_reply": "2024-04-13T19:27:00.333327Z"
    },
    "papermill": {
     "duration": 1245.663348,
     "end_time": "2024-04-13T19:27:00.334448",
     "exception": false,
     "start_time": "2024-04-13T19:06:14.671100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started script\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   7%|███████████▏                                                                                                                                                            | 1/15 [01:27<20:25, 87.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  13%|██████████████████████▍                                                                                                                                                 | 2/15 [02:55<19:00, 87.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  20%|█████████████████████████████████▌                                                                                                                                      | 3/15 [04:23<17:34, 87.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  27%|████████████████████████████████████████████▊                                                                                                                           | 4/15 [05:49<15:59, 87.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  33%|████████████████████████████████████████████████████████                                                                                                                | 5/15 [07:17<14:34, 87.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  40%|███████████████████████████████████████████████████████████████████▏                                                                                                    | 6/15 [08:44<13:04, 87.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  47%|██████████████████████████████████████████████████████████████████████████████▍                                                                                         | 7/15 [10:20<12:00, 90.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  53%|█████████████████████████████████████████████████████████████████████████████████████████▌                                                                              | 8/15 [11:51<10:33, 90.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                   | 9/15 [13:18<08:56, 89.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 10/15 [14:45<07:23, 88.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                            | 11/15 [16:12<05:53, 88.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 12/15 [17:44<04:28, 89.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 13/15 [19:09<02:55, 87.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 14/15 [20:33<01:26, 86.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [20:38<00:00, 62.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [20:38<00:00, 82.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"started script\")\n",
    "\n",
    "# llama_path = \"h2oai/h2ogpt-4096-llama2-7b-chat\"\n",
    "llama_path = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "# llama_path = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_path, \n",
    "                                          token = token,\n",
    "                                          cache_dir = \"../hf_cache/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_path, \n",
    "                                             token = token,\n",
    "                                             device_map = \"auto\",\n",
    "                                            # device_map = \"cuda:1\",\n",
    "                                             torch_dtype = torch_dtype,\n",
    "                                             cache_dir = \"../hf_cache/\")\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bff68",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb16231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:27:00.362416Z",
     "iopub.status.busy": "2024-04-13T19:27:00.362252Z",
     "iopub.status.idle": "2024-04-13T19:27:15.706708Z",
     "shell.execute_reply": "2024-04-13T19:27:15.706179Z"
    },
    "papermill": {
     "duration": 15.366867,
     "end_time": "2024-04-13T19:27:15.707357",
     "exception": true,
     "start_time": "2024-04-13T19:27:00.340490",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m tokenized_prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(medmcqa2, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m time_before \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 15\u001b[0m raw_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(tokenized_prompt,\n\u001b[1;32m     16\u001b[0m                             max_new_tokens \u001b[38;5;241m=\u001b[39m max_new_tokens,\n\u001b[1;32m     17\u001b[0m                             temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     18\u001b[0m time_after \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     19\u001b[0m time_taken \u001b[38;5;241m=\u001b[39m time_after \u001b[38;5;241m-\u001b[39m time_before\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/transformers/generation/utils.py:1575\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1568\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1569\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1570\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1572\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1575\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   1576\u001b[0m         input_ids,\n\u001b[1;32m   1577\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1578\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[1;32m   1579\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1580\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1581\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m   1582\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m   1583\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[1;32m   1584\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1585\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1586\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1587\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1588\u001b[0m     )\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1593\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1594\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1600\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/transformers/generation/utils.py:2735\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2734\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2735\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "max_new_tokens = 150\n",
    "prompt = \"In the era of generative AI, \"\n",
    "medmcqa2 = \"\"\"\n",
    "Question.\n",
    "Low insulin to glucagon ratio is not seen in:\n",
    "(A) Glycogen synthesis\n",
    "(B) Glycogen breakdown\n",
    "(C) Gluconeogenesis\n",
    "(D) Ketogenesis\n",
    "Answer. \n",
    "\"\"\"\n",
    "tokenized_prompt = tokenizer.encode(medmcqa2, return_tensors = \"pt\").to('cuda:1')\n",
    "time_before = time()\n",
    "raw_output = model.generate(tokenized_prompt,\n",
    "                            max_new_tokens = max_new_tokens,\n",
    "                            temperature = 0.01)\n",
    "time_after = time()\n",
    "time_taken = time_after - time_before\n",
    "untokenized_output = tokenizer.decode(raw_output[0], skip_special_tokens = True)\n",
    "num_generated = len(raw_output[0]) - len(tokenized_prompt[0])\n",
    "print(untokenized_output)\n",
    "print(f\"newly generated {num_generated}\")\n",
    "print(f\"{num_generated / time_taken} t/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d8afa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + \"/pubmed_cleaned/\"\n",
    "print(path)\n",
    "files = os.listdir(path)\n",
    "print(files[:5])\n",
    "# load in the first file and print first 20 lines\n",
    "# with open(path + files[0], \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines[:20]:\n",
    "#         print(line)\n",
    "from datasets import load_dataset\n",
    "print(path + files[0])\n",
    "dataset = load_dataset(\"text\", \n",
    "                       cache_dir = \"../hf_cache/\",\n",
    "                       data_files = path + files[0], \n",
    "                       split = \"train\")\n",
    "# dataset = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9467397",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15518365",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = True\n",
    "model.model.embed_tokens.weight.requires_grad_(False)\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264bb51a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659492dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "batch_size = 2\n",
    "total_num_steps = 500000\n",
    "output_dir = \"../\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    # per_device_train_batch_size=batch_size,\n",
    "    # per_device_eval_batch_size=batch_size//2,\n",
    "    # bf16=False,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=1,\n",
    "    # max_steps = -1\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=total_num_steps // 6,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67ebcc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt(row):\n",
    "    text = row[\"text\"]\n",
    "    return text\n",
    "# create_prompt(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd769f4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97cd85",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab20bd0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    # eval_dataset=test_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=512,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2a666",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set llama model config use_cache to false!!!\n",
    "trainer.train()\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f56f86",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.save_model(output_dir)\n",
    "# #print contents of output_dir\n",
    "# !ls -l $output_dir\n",
    "# #print full path of output_dir\n",
    "# # !pwd $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1456a7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a966a6f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, data, tokenizer):\n",
    "#         self.data = data\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.data[idx]['text']\n",
    "#         inputs = self.tokenizer(text, \n",
    "#                                 return_tensors=\"pt\", \n",
    "#                                 padding=True, \n",
    "#                                 max_length=10,\n",
    "#                                 truncation=True)\n",
    "#         return inputs\n",
    "# text_dataset = TextDataset(dataset, tokenizer)\n",
    "# dataloader = DataLoader(text_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b6d09",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# def train(model, dataloader, epochs=5, lr=5e-5):\n",
    "#     device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     # model.to(device)\n",
    "\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "#     print(\"starting training\")\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         for item in dataset:\n",
    "#             print(len(item))\n",
    "#             text = item[\"text\"]\n",
    "#             inputs = tokenizer(text, return_tensors=\"pt\", padding=True, max_length=10, truncation=True)\n",
    "#             # print(batch)\n",
    "#             inputs = inputs.to(device)\n",
    "#             print(inputs.input_ids.shape)\n",
    "#             print(inputs)\n",
    "#             outputs = model(**inputs, labels=inputs.input_ids)\n",
    "#             loss = outputs.loss\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce031a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b24ff3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9608ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# train(model, \"dataloader\", epochs=5, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f98de",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ade18b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1274.39654,
   "end_time": "2024-04-13T19:27:21.193134",
   "environment_variables": {},
   "exception": true,
   "input_path": "experimentation.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-04-13T19:06:06.796594",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}