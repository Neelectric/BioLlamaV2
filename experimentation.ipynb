{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/miniconda3/lib/python3.10/site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.10/site-packages (from accelerate) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/miniconda3/lib/python3.10/site-packages (from accelerate) (0.20.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git config --global credential.helper store\n",
    "!pip install accelerate\n",
    "!git config --global user.name \"Neelectric\"\n",
    "!git config --global user.email \"Neel.R@web.de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = input(f\"Enter token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.biollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started script\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was a problem when trying to write in your cache folder (/root/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e8b8bdd29b4116b8b57179d19adb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"started script\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "print(\"imported...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", \n",
    "                                          token = token,\n",
    "                                          cache_dir = \"../hf_cache/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", \n",
    "                                             token = token,\n",
    "                                             device_map = \"auto\",\n",
    "                                             torch_dtype = torch.float16,\n",
    "                                             cache_dir = \"../hf_cache/\")\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question.\n",
      "Low insulin to glucagon ratio is not seen in:\n",
      "(A) Glycogen synthesis\n",
      "(B) Glycogen breakdown\n",
      "(C) Gluconeogenesis\n",
      "(D) Ketogenesis\n",
      "Answer. \n",
      "Let's think step by step. To this end, let's first reflect on the question and which concepts will be relevant in answering it. Then, let's brainstorm on how concepts in the answer options relate back to it. Finally, let's evaluate each option in relation to the question, choosing one.\n",
      "\n",
      "Answer: C\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The question asks about low insulin to glucagon ratio, which is relevant to glucose metabolism. Glucagon is a hormone that raises blood glucose levels by stimulating glycogen breakdown and gluconeogenesis. Insulin, on the other hand, lowers blood glucose levels by promoting glycogen synthesis and inhibiting gluconeogenesis.\n",
      "\n",
      "Now, let's evaluate each answer option in relation to the question:\n",
      "\n",
      "A) Glycogen synthesis: Insulin promotes glycogen synthesis, so a low insulin to glucagon ratio would not be associated with glycogen synthesis.\n",
      "\n",
      "B) Glycogen breakdown: Glucagon stimulates glycogen breakdown, so a low insulin to glucagon ratio would be associated with glycogen breakdown. Correct!\n",
      "\n",
      "C) Gluconeogenesis: Insulin inhibits gluconeogenesis, so a low insulin to glucagon ratio would be associated with gluconeogenesis. Correct!\n",
      "\n",
      "D) Ketogenesis: Insulin does not affect ketogenesis, so a low insulin to glucagon ratio would not be associated with ketogenesis.\n",
      "\n",
      "Therefore, the correct answer is C) Gluconeogenesis.\n",
      "newly generated 325\n",
      "19.44474911216166 t/s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "max_new_tokens = 400\n",
    "prompt = \"In the era of generative AI, \"\n",
    "medmcqa2 = \"\"\"\n",
    "Question.\n",
    "Low insulin to glucagon ratio is not seen in:\n",
    "(A) Glycogen synthesis\n",
    "(B) Glycogen breakdown\n",
    "(C) Gluconeogenesis\n",
    "(D) Ketogenesis\n",
    "Answer. \n",
    "Let's think step by step. To this end, let's first reflect on the question and which concepts will be relevant in answering it. Then, let's brainstorm on how concepts in the answer options relate back to it. Finally, let's evaluate each option in relation to the question, choosing one.\n",
    "\"\"\"\n",
    "tokenized_prompt = tokenizer.encode(medmcqa2, return_tensors = \"pt\").to('cuda')\n",
    "time_before = time()\n",
    "raw_output = model.generate(tokenized_prompt,\n",
    "                            # max_new_tokens = max_new_tokens,\n",
    "                            temperature = 0.01)\n",
    "time_after = time()\n",
    "time_taken = time_after - time_before\n",
    "untokenized_output = tokenizer.decode(raw_output[0], skip_special_tokens = True)\n",
    "num_generated = len(raw_output[0]) - len(tokenized_prompt[0])\n",
    "print(untokenized_output)\n",
    "print(f\"newly generated {num_generated}\")\n",
    "print(f\"{num_generated / time_taken} t/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "medmcqa = \"\"\"\n",
    "Question:\n",
    "Axonal transport is:\n",
    "(A) Antegrade\n",
    "(B) Retrograde\n",
    "(C) Antegrade and retrograde\n",
    "(D) None\n",
    "Answer: Let's reflect on each response individually, without choosing whether it is correct yet. Then, discuss the responses in relation to the question, hypothesizing on whether it might be the right answer. Finally choose one from these candidates after considering them carefully.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
