{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=../hf_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git config --global credential.helper store\n",
    "!pip install accelerate\n",
    "!git config --global user.name \"Neelectric\"\n",
    "!git config --global user.email \"Neel.R@web.de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "# wandb.init(project=\"biollama_v2\", # the project I am working on\n",
    "#            tags=[\"hf_sft\", \"BioLlamaV2\"]) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# wandb.alert(\n",
    "#     title=\"Initialising training run\",\n",
    "#     text=f\"We have started training\",\n",
    "#     level=AlertLevel.WARN,\n",
    "#     wait_duration=300,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported...\n",
      "4.39.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "print(\"imported...\")\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pull the huggingface token from the local config. Remember to make a copy of config_default.yml, name it config.yml and add your huggingface token. config.yml is in the gitignore so unless you misspell config.yml, it should not get pushed to github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = input(f\"Enter token: \")\n",
    "from box import Box\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = Box.from_yaml(f.read())\n",
    "token = config.secrets.hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to verfiy whether the GPU supports bfloat16 (less precision but more range). If not (for example if running on TITAN GPUs), we use float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_bf16_supported():\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "torch_dtype = torch.bfloat16\n",
    "print(f\"Using dtype: {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n",
    "# medcpt_path = \"ncbi/MedCPT-Article-Encoder\"\n",
    "# document_tokenizer = AutoTokenizer.from_pretrained(medcpt_path)\n",
    "# document_model = AutoModel.from_pretrained(medcpt_path).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started script\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"started script\")\n",
    "\n",
    "llama_path = \"h2oai/h2ogpt-4096-llama2-7b-chat\"\n",
    "# llama_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# llama_path = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_path, \n",
    "                                          token = token,\n",
    "                                          cache_dir = \"../hf_cache/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_path, \n",
    "                                             token = token,\n",
    "                                            #  device_map = \"auto\",\n",
    "                                            device_map = \"cuda:1\",\n",
    "                                             torch_dtype = torch_dtype,\n",
    "                                             cache_dir = \"../hf_cache/\")\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question.\n",
      "Low insulin to glucagon ratio is not seen in:\n",
      "(A) Glycogen synthesis\n",
      "(B) Glycogen breakdown\n",
      "(C) Gluconeogenesis\n",
      "(D) Ketogenesis\n",
      "Answer. \n",
      "(D) Ketogenesis\n",
      "\n",
      "Explanation:\n",
      "A low insulin to glucagon ratio is typically seen in states of ketosis, where the body is relying on ketones for energy instead of glucose. Glucagon is a hormone that raises blood glucose levels by stimulating the breakdown of glycogen and fatty acids, and it is inversely related to insulin. Therefore, a low insulin to glucagon ratio indicates that the body is in a state of ketosis, where ketones are being used as the primary source of energy. Glycogen synthesis, glycogen breakdown,\n",
      "newly generated 150\n",
      "30.138704158394212 t/s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "max_new_tokens = 150\n",
    "prompt = \"In the era of generative AI, \"\n",
    "medmcqa2 = \"\"\"\n",
    "Question.\n",
    "Low insulin to glucagon ratio is not seen in:\n",
    "(A) Glycogen synthesis\n",
    "(B) Glycogen breakdown\n",
    "(C) Gluconeogenesis\n",
    "(D) Ketogenesis\n",
    "Answer. \n",
    "\"\"\"\n",
    "tokenized_prompt = tokenizer.encode(medmcqa2, return_tensors = \"pt\").to('cuda:1')\n",
    "time_before = time()\n",
    "raw_output = model.generate(tokenized_prompt,\n",
    "                            max_new_tokens = max_new_tokens,\n",
    "                            temperature = 0.01)\n",
    "time_after = time()\n",
    "time_taken = time_after - time_before\n",
    "untokenized_output = tokenizer.decode(raw_output[0], skip_special_tokens = True)\n",
    "num_generated = len(raw_output[0]) - len(tokenized_prompt[0])\n",
    "print(untokenized_output)\n",
    "print(f\"newly generated {num_generated}\")\n",
    "print(f\"{num_generated / time_taken} t/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/primary/pubmed_cleaned/\n",
      "['abs_1_14.tsv', 'abs_1_15.tsv', 'abs_1_23.tsv', 'abs_1_0.tsv', 'abs_1_30.tsv']\n",
      "/nfs/primary/pubmed_cleaned/abs_1_14.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + \"/pubmed_cleaned/\"\n",
    "print(path)\n",
    "files = os.listdir(path)\n",
    "print(files[:5])\n",
    "# load in the first file and print first 20 lines\n",
    "# with open(path + files[0], \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines[:20]:\n",
    "#         print(line)\n",
    "from datasets import load_dataset\n",
    "print(path + files[0])\n",
    "dataset = load_dataset(\"text\", \n",
    "                       cache_dir = \"../hf_cache/\",\n",
    "                       data_files = path + files[0], \n",
    "                       split = \"train\")\n",
    "# dataset = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 67773\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 6607.34M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = True\n",
    "model.model.embed_tokens.weight.requires_grad_(False)\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trl import SFTTrainer\n",
    "# from transformers import TrainingArguments\n",
    "# batch_size = 2\n",
    "# total_num_steps = 50\n",
    "# output_dir = \"../\"\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     # per_device_train_batch_size=batch_size,\n",
    "#     # per_device_eval_batch_size=batch_size//2,\n",
    "#     # bf16=False,\n",
    "#     learning_rate=2e-4,\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     warmup_steps=total_num_steps // 10,\n",
    "#     num_train_epochs=1,\n",
    "#     # max_steps = -1\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     gradient_checkpointing=True,\n",
    "#     # evaluation_strategy=\"steps\",\n",
    "#     # eval_steps=total_num_steps // 6,\n",
    "#     # logging strategies\n",
    "#     logging_dir=f\"{output_dir}/logs\",\n",
    "#     logging_strategy=\"steps\",\n",
    "#     logging_steps=1,\n",
    "#     save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    "#     save_total_limit=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_prompt(row):\n",
    "#     text = row[\"text\"]\n",
    "#     return text\n",
    "# # create_prompt(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = SFTTrainer(\n",
    "#     model,\n",
    "#     train_dataset=train_dataset,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     # eval_dataset=test_dataset,\n",
    "#     packing=True,\n",
    "#     max_seq_length=512,\n",
    "#     args=training_args,\n",
    "#     formatting_func=create_prompt,\n",
    "#     # compute_metrics=token_accuracy,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #set llama model config use_cache to false!!!\n",
    "# trainer.train()\n",
    "# # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(output_dir)\n",
    "# #print contents of output_dir\n",
    "# !ls -l $output_dir\n",
    "# #print full path of output_dir\n",
    "# # !pwd $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, data, tokenizer):\n",
    "#         self.data = data\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.data[idx]['text']\n",
    "#         inputs = self.tokenizer(text, \n",
    "#                                 return_tensors=\"pt\", \n",
    "#                                 padding=True, \n",
    "#                                 max_length=10,\n",
    "#                                 truncation=True)\n",
    "#         return inputs\n",
    "# text_dataset = TextDataset(dataset, tokenizer)\n",
    "# dataloader = DataLoader(text_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, dataloader, epochs=5, lr=5e-5):\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    print(\"starting training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for item in dataset:\n",
    "            text = item[\"text\"]\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, max_length=10, truncation=True)\n",
    "            # print(batch)\n",
    "            inputs = inputs.to(device)\n",
    "            print(inputs.input_ids.shape)\n",
    "            print(inputs)\n",
    "            outputs = model(**inputs, labels=inputs.input_ids)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tTwenty male patients with primary hypertriglyceridemia were treated for 4 weeks with daily supplements (15 g) of oil, which provided approximately 6 g of polyunsaturated fatty acid (PUFA) either of fish or of vegetable origin. Total plasma cholesterol concentrations were unaffected, but both types of supplement increased high density lipoprotein-3 (HDL3) cholesterol concentrations. The fish, but not the vegetable, oil supplement led to a decrease in plasma triglyceride concentrations. Very low density lipoprotein (VLDL), fatty acid composition, and VLDL triglyceride kinetics were subsequently studied in five patients (four male, one female) before and after 4 weeks of therapy with 15 g of the same fish oil. The fish oil led to increases in the proportion of eicosapentaenoic acid in both the VLDL triglyceride and phospholipid fractions, but the increase was greater in the latter. In contrast, the proportion of docosahexanoic acid was increased only in the VLDL triglycerides. The decrease in plasma triglyceride concentrations that occurred with fish-oil therapy was accompanied by a reduction in the absolute catabolic rate of VLDL triglyceride, implying a concomitant change in synthetic rate; the fractional catabolic rate of VLDL triglyceride was unaltered. It is suggested that polyunsaturated fatty acids of marine origin may be therapeutically useful for hypertriglyceridemia.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "torch.Size([1, 10])\n",
      "{'input_ids': tensor([[    1, 29871,    12, 27418,  6478, 14263, 22069,   411,  7601, 11266]],\n",
      "       device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1')}\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 1 has a total capacity of 47.54 GiB of which 7.62 MiB is free. Process 59195 has 47.53 GiB memory in use. Of the allocated memory 46.30 GiB is allocated by PyTorch, and 59.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m----> 2\u001b[0m train(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader\u001b[39m\u001b[38;5;124m\"\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, epochs, lr)\u001b[0m\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/torch/optim/adamw.py:176\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    173\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    174\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 176\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    177\u001b[0m         group,\n\u001b[1;32m    178\u001b[0m         params_with_grad,\n\u001b[1;32m    179\u001b[0m         grads,\n\u001b[1;32m    180\u001b[0m         amsgrad,\n\u001b[1;32m    181\u001b[0m         exp_avgs,\n\u001b[1;32m    182\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    183\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    184\u001b[0m         state_steps,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    187\u001b[0m     adamw(\n\u001b[1;32m    188\u001b[0m         params_with_grad,\n\u001b[1;32m    189\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/torch/optim/adamw.py:123\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    117\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    118\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    124\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    127\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    128\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    129\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 1 has a total capacity of 47.54 GiB of which 7.62 MiB is free. Process 59195 has 47.53 GiB memory in use. Of the allocated memory 46.30 GiB is allocated by PyTorch, and 59.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train(model, \"dataloader\", epochs=5, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
