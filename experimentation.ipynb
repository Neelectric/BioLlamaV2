{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=../hf_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/envs/biollama/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git config --global credential.helper store\n",
    "!pip install accelerate\n",
    "!git config --global user.name \"Neelectric\"\n",
    "!git config --global user.email \"Neel.R@web.de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnelectric\u001b[0m (\u001b[33mneelectric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/primary/BioLlamaV2/wandb/run-20240409_173324-74mdlfx2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/neelectric/biollama_v2/runs/74mdlfx2/workspace' target=\"_blank\">hearty-shadow-11</a></strong> to <a href='https://wandb.ai/neelectric/biollama_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/neelectric/biollama_v2' target=\"_blank\">https://wandb.ai/neelectric/biollama_v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/neelectric/biollama_v2/runs/74mdlfx2/workspace' target=\"_blank\">https://wandb.ai/neelectric/biollama_v2/runs/74mdlfx2/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "wandb.init(project=\"biollama_v2\", # the project I am working on\n",
    "           tags=[\"hf_sft\", \"BioLlamaV2\"]) # the Hyperparameters I want to keep track of\n",
    "\n",
    "wandb.alert(\n",
    "    title=\"Initialising training run\",\n",
    "    text=f\"We have started training\",\n",
    "    level=AlertLevel.WARN,\n",
    "    wait_duration=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported...\n",
      "4.39.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "print(\"imported...\")\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pull the huggingface token from the local config. Remember to make a copy of config_default.yml, name it config.yml and add your huggingface token. config.yml is in the gitignore so unless you misspell config.yml, it should not get pushed to github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = input(f\"Enter token: \")\n",
    "from box import Box\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = Box.from_yaml(f.read())\n",
    "token = config.secrets.hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to verfiy whether the GPU supports bfloat16 (less precision but more range). If not (for example if running on TITAN GPUs), we use float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_bf16_supported():\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "torch_dtype = torch.float32\n",
    "print(f\"Using dtype: {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started script\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"started script\")\n",
    "\n",
    "llama_path = \"h2oai/h2ogpt-4096-llama2-7b-chat\"\n",
    "# llama_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# llama_path = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_path, \n",
    "                                          token = token,\n",
    "                                          cache_dir = \"../hf_cache/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_path, \n",
    "                                             token = token,\n",
    "                                            #  device_map = \"auto\",\n",
    "                                            device_map = \"cuda:0\",\n",
    "                                             torch_dtype = torch_dtype,\n",
    "                                             cache_dir = \"../hf_cache/\")\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question.\n",
      "Low insulin to glucagon ratio is not seen in:\n",
      "(A) Glycogen synthesis\n",
      "(B) Glycogen breakdown\n",
      "(C) Gluconeogenesis\n",
      "(D) Ketogenesis\n",
      "Answer. \n",
      "(D) Ketogenesis\n",
      "\n",
      "Explanation:\n",
      "A low insulin to glucagon ratio is typically seen in states of ketosis, where the body is relying on ketones for energy instead of glucose. Therefore, option (D) Ketogenesis is the correct answer.\n",
      "Glycogen synthesis (A) and glycogen breakdown (B) are processes that occur in the liver and are regulated by insulin and glucagon, respectively. Gluconeogenesis (C) is the process by which the liver and kidneys produce glucose from non-carbohydrate sources, such as amino ac\n",
      "newly generated 150\n",
      "20.485244009078365 t/s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "max_new_tokens = 150\n",
    "prompt = \"In the era of generative AI, \"\n",
    "medmcqa2 = \"\"\"\n",
    "Question.\n",
    "Low insulin to glucagon ratio is not seen in:\n",
    "(A) Glycogen synthesis\n",
    "(B) Glycogen breakdown\n",
    "(C) Gluconeogenesis\n",
    "(D) Ketogenesis\n",
    "Answer. \n",
    "\"\"\"\n",
    "tokenized_prompt = tokenizer.encode(medmcqa2, return_tensors = \"pt\").to('cuda')\n",
    "time_before = time()\n",
    "raw_output = model.generate(tokenized_prompt,\n",
    "                            max_new_tokens = max_new_tokens,\n",
    "                            temperature = 0.01)\n",
    "time_after = time()\n",
    "time_taken = time_after - time_before\n",
    "untokenized_output = tokenizer.decode(raw_output[0], skip_special_tokens = True)\n",
    "num_generated = len(raw_output[0]) - len(tokenized_prompt[0])\n",
    "print(untokenized_output)\n",
    "print(f\"newly generated {num_generated}\")\n",
    "print(f\"{num_generated / time_taken} t/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/primary/pubmed_cleaned/\n",
      "['abs_1_14.tsv', 'abs_1_15.tsv', 'abs_1_23.tsv', 'abs_1_0.tsv', 'abs_1_30.tsv']\n",
      "/nfs/primary/pubmed_cleaned/abs_1_14.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + \"/pubmed_cleaned/\"\n",
    "print(path)\n",
    "files = os.listdir(path)\n",
    "print(files[:5])\n",
    "# load in the first file and print first 20 lines\n",
    "# with open(path + files[0], \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines[:20]:\n",
    "#         print(line)\n",
    "from datasets import load_dataset\n",
    "print(path + files[0])\n",
    "dataset = load_dataset(\"text\", \n",
    "                       cache_dir = \"../hf_cache/\",\n",
    "                       data_files = path + files[0], \n",
    "                       split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 67773\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 6607.34M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = True\n",
    "model.model.embed_tokens.weight.requires_grad_(False);\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "batch_size = 2\n",
    "total_num_steps = 50\n",
    "output_dir = \"../\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    # per_device_train_batch_size=batch_size,\n",
    "    # per_device_eval_batch_size=batch_size//2,\n",
    "    # bf16=False,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=total_num_steps // 10,\n",
    "    num_train_epochs=2,\n",
    "    # max_steps = -1\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=total_num_steps // 6,\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\", #changed to epoch so we save every epoch i guess?\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(row):\n",
    "    text = row[\"text\"]\n",
    "    return text\n",
    "# create_prompt(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/biollama/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    # eval_dataset=test_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=512,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    # compute_metrics=token_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    }
   ],
   "source": [
    "#set llama model config use_cache to false!!!\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "#print contents of output_dir\n",
    "!ls -l $output_dir\n",
    "#print full path of output_dir\n",
    "# !pwd $output_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
